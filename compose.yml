version: "3.9"
services:

  pod-llama-cpp-swap:
    build:
      context: env-llama-cpp-swap/
      dockerfile: Containerfile
    ports:
      - "8686:8686"
    devices:
      - "nvidia.com/gpu=all"
    volumes:
      - ${HOST_CACHE_HUGGINGFACE:-~/.cache/huggingface}:/root/.cache/huggingface
      - ${HOST_CACHE_LLAMACPP:-~/.cache/llama.cpp}:/root/.cache/llama.cpp
      - ./config-llamacpp-container.yaml:/root/llama-swap-config.yaml
    security_opt:
      - label:disable
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN
    ipc: host
    entrypoint:
      /usr/local/bin/llama-swap
    command:
      -config /root/llama-swap-config.yaml -listen ':8686'

  pod-tabbyapi:
    image: ghcr.io/theroyallab/tabbyapi:latest
    ports:
      - "5000:5000"
    devices:
      - "nvidia.com/gpu=all"
    volumes:
      - ${HOST_CACHE_HUGGINGFACE:-~/.cache/huggingface/hub}:/app/models
      - ./tabby-api_tokens.yml:/app/api_tokens.yml
      - ./tabby-config-qwq32b.yml:/app/config.yml
    security_opt:
      - label:disable
    environment:
      - NAME=TabbyAPI
    ipc: host   

  pod-litellm-proxy:
    build:
      context: env-litellm-patched/
      dockerfile: Containerfile
    ports:
      - "4000:4000"
    volumes:
      - ./litellm.yml:/root/litellm.yml
    command:
      host-litellm.py --config /root/litellm.yml # --detailed_debug
