version: "3.9"
services:

  pod-llama-swap:
    build:
      context: env-llama-swap/
      dockerfile: Containerfile
    ports:
      - "8686:8686"
    devices:
      - "nvidia.com/gpu=all"
    volumes:
      - ${HOST_CACHE_HUGGINGFACE:-~/.cache/huggingface}:/root/.cache/huggingface
      - ${HOST_CACHE_HUGGINGFACE:-~/.cache/huggingface}/hub:/models  # tabbyAPI...
      - ${HOST_CACHE_LLAMACPP:-~/.cache/llama.cpp}:/root/.cache/llama.cpp
      - ./llama-swap-config.yaml:/root/llama-swap-config.yaml
      - ./tabby-api_tokens.yml:/api_tokens.yml
      - ./tabby-config-qwq32b.yml:/opt/tabby-config-qwq32b.yml
      - ./tabby-config-qwen25-coder.yml:/opt/tabby-config-qwen25-coder.yml
    working_dir: "/"
    security_opt:
      - label:disable
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN
    ipc: host
    entrypoint:
      /usr/local/bin/llama-swap
    command:
      -config /root/llama-swap-config.yaml -listen ':8686'

  pod-litellm-proxy:
    build:
      context: env-litellm-patched/
      dockerfile: Containerfile
    ports:
      - "4000:4000"
    volumes:
      - ./litellm.yml:/root/litellm.yml
      - ./host-litellm.py:/root/host-litellm.py
    command:
      /root/host-litellm.py --config /root/litellm.yml # --detailed_debug
    depends_on:
      - pod-llama-swap
