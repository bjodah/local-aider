model_list:
  - model_name: "local-cpp-qwq-32b"  # "huggingface/unsloth/QwQ-32B-GGUF"
    litellm_params:
      model: "openai/cpp-unsloth--QwQ-32B-GGUF" #openai/QwQ-32B
      api_base: http://pod-llama-swap:8686/v1
      api_key: "your-llamacpp-api-key"   # if --api-key flag is passed to LLaMA.cpp's HTTP Server
      rpm: 60
      max_tokens: 32768
  - model_name: "local-cpp-qwen25-coder-32b" # "huggingface/unsloth/Qwen2.5-Coder-32B-Instruct-GGUF"
    litellm_params:
      model: "openai/cpp-unsloth--Qwen2.5-Coder-32B-Instruct-GGUF"  #openai/Qwen2.5-Coder-32B
      api_base: http://pod-llama-swap:8686/v1
      api_key: "your-llamacpp-api-key"   # if --api-key flag is passed to LLaMA.cpp's HTTP Server
      num_retries: 1
      rpm: 600
      max_tokens: 32768

  - model_name: "local-exllamav2-qwq-32b"
    litellm_params:
      model: "openai/exllamav2-bartowski-qwq-32b"
      api_base: http://pod-llama-swap:8686/v1
      api_key: "your-tabby-api-key"
      num_retries: 1
      rpm: 60
      max_tokens: 32768
      top_p: 0.95
      top_k: 40
      min_p: 0.01
      dry_allowed_length: 5
      dry_multiplier: 0.5
      repetition_penalty: 1.1
      temperature: 0.8
  #     logit_bias: {"151668": 5}  # "</think>"

  - model_name: "local-exllamav2-qwen25-coder-32b"
    litellm_params:
      model: "openai/exllamav2-bartowski-qwen25-coder-32b"
      api_base: http://pod-llama-swap:8686/v1
      api_key: "your-tabby-api-key"
      num_retries: 1
      rpm: 60
      max_tokens: 32768
      top_p: 0.95
      top_k: 40
      min_p: 0.01
      dry_allowed_length: 5
      dry_multiplier: 0.5
      repetition_penalty: 1.1
      temperature: 0.3

  - model_name: "local-vllm-qwq-32b"
    litellm_params:
      model: "openai/vllm-unsloth-qwq-32b-bnb"
      api_base: http://pod-llama-swap:8686/v1
      api_key: "your-vllm-api-key"
      num_retries: 1
      rpm: 60
      max_tokens: 32768

general_settings:
  master_key: sk-deadbeef0badcafe


